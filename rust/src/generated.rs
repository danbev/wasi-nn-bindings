// Generated by `wit-bindgen` 0.22.0. DO NOT EDIT!
// Options used:
pub mod wasi {
  pub mod nn {
    #[allow(clippy::all)]
    pub mod tensor {
      #[used]
      #[doc(hidden)]
      #[cfg(target_arch = "wasm32")]
      static __FORCE_SECTION_REF: fn() = super::super::super::__link_custom_section_describing_imports;
      use super::super::super::_rt;
      /// The dimensions of a tensor.
      ///
      /// The array length matches the tensor rank and each element in the array describes the size of
      /// each dimension
      pub type TensorDimensions = _rt::Vec::<u32>;
      /// The type of the elements in a tensor.
      #[repr(u8)]
      #[derive(Clone, Copy, Eq, PartialEq)]
      pub enum TensorType {
        Fp16,
        Fp32,
        Fp64,
        Bf16,
        U8,
        I32,
        I64,
      }
      impl ::core::fmt::Debug for TensorType {
        fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
          match self {
            TensorType::Fp16 => {
              f.debug_tuple("TensorType::Fp16").finish()
            }
            TensorType::Fp32 => {
              f.debug_tuple("TensorType::Fp32").finish()
            }
            TensorType::Fp64 => {
              f.debug_tuple("TensorType::Fp64").finish()
            }
            TensorType::Bf16 => {
              f.debug_tuple("TensorType::Bf16").finish()
            }
            TensorType::U8 => {
              f.debug_tuple("TensorType::U8").finish()
            }
            TensorType::I32 => {
              f.debug_tuple("TensorType::I32").finish()
            }
            TensorType::I64 => {
              f.debug_tuple("TensorType::I64").finish()
            }
          }
        }
      }

      impl TensorType{
        pub(crate) unsafe fn _lift(val: u8) -> TensorType{
          if !cfg!(debug_assertions) {
            return ::core::mem::transmute(val);
          }

          match val {
            0 => TensorType::Fp16,
            1 => TensorType::Fp32,
            2 => TensorType::Fp64,
            3 => TensorType::Bf16,
            4 => TensorType::U8,
            5 => TensorType::I32,
            6 => TensorType::I64,

            _ => panic!("invalid enum discriminant"),
          }
        }
      }

      /// The tensor data.
      ///
      /// Initially conceived as a sparse representation, each empty cell would be filled with zeros
      /// and the array length must match the product of all of the dimensions and the number of bytes
      /// in the type (e.g., a 2x2 tensor with 4-byte f32 elements would have a data array of length
      /// 16). Naturally, this representation requires some knowledge of how to lay out data in
      /// memory--e.g., using row-major ordering--and could perhaps be improved.
      pub type TensorData = _rt::Vec::<u8>;
      #[derive(Clone)]
      pub struct Tensor {
        /// Describe the size of the tensor (e.g., 2x2x2x2 -> [2, 2, 2, 2]). To represent a tensor
        /// containing a single value, use `[1]` for the tensor dimensions.
        pub dimensions: TensorDimensions,
        /// Describe the type of element in the tensor (e.g., `f32`).
        pub tensor_type: TensorType,
        /// Contains the tensor data.
        pub data: TensorData,
      }
      impl ::core::fmt::Debug for Tensor {
        fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
          f.debug_struct("Tensor").field("dimensions", &self.dimensions).field("tensor-type", &self.tensor_type).field("data", &self.data).finish()
        }
      }

    }

    #[allow(clippy::all)]
    pub mod errors {
      #[used]
      #[doc(hidden)]
      #[cfg(target_arch = "wasm32")]
      static __FORCE_SECTION_REF: fn() = super::super::super::__link_custom_section_describing_imports;
      #[repr(u8)]
      #[derive(Clone, Copy, Eq, PartialEq)]
      pub enum Error {
        /// Caller module passed an invalid argument.
        InvalidArgument,
        /// Invalid encoding.
        InvalidEncoding,
        Busy,
        /// Runtime Error.
        RuntimeError,
        /// Unsupported operation.
        UnsupportedOperation,
        /// Graph is too large.
        TooLarge,
        /// Graph not found.
        NotFound,
      }
      impl Error{
        pub fn name(&self) -> &'static str {
          match self {
            Error::InvalidArgument => "invalid-argument",
            Error::InvalidEncoding => "invalid-encoding",
            Error::Busy => "busy",
            Error::RuntimeError => "runtime-error",
            Error::UnsupportedOperation => "unsupported-operation",
            Error::TooLarge => "too-large",
            Error::NotFound => "not-found",
          }
        }
        pub fn message(&self) -> &'static str {
          match self {
            Error::InvalidArgument => "Caller module passed an invalid argument.",
            Error::InvalidEncoding => "Invalid encoding.",
            Error::Busy => "",
            Error::RuntimeError => "Runtime Error.",
            Error::UnsupportedOperation => "Unsupported operation.",
            Error::TooLarge => "Graph is too large.",
            Error::NotFound => "Graph not found.",
          }
        }
      }
      impl ::core::fmt::Debug for Error{
        fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
          f.debug_struct("Error")
          .field("code", &(*self as i32))
          .field("name", &self.name())
          .field("message", &self.message())
          .finish()
        }
      }
      impl ::core::fmt::Display for Error{
        fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
          write!(f, "{} (error {})", self.name(), *self as i32)
        }
      }

      impl std::error::Error for Error {}

      impl Error{
        pub(crate) unsafe fn _lift(val: u8) -> Error{
          if !cfg!(debug_assertions) {
            return ::core::mem::transmute(val);
          }

          match val {
            0 => Error::InvalidArgument,
            1 => Error::InvalidEncoding,
            2 => Error::Busy,
            3 => Error::RuntimeError,
            4 => Error::UnsupportedOperation,
            5 => Error::TooLarge,
            6 => Error::NotFound,

            _ => panic!("invalid enum discriminant"),
          }
        }
      }


    }

    #[allow(clippy::all)]
    pub mod graph {
      #[used]
      #[doc(hidden)]
      #[cfg(target_arch = "wasm32")]
      static __FORCE_SECTION_REF: fn() = super::super::super::__link_custom_section_describing_imports;
      use super::super::super::_rt;
      pub type Error = super::super::super::wasi::nn::errors::Error;
      /// An execution graph for performing inference (i.e., a model).
      ///
      /// TODO: replace with `resource` (https://github.com/WebAssembly/wasi-nn/issues/47).
      pub type Graph = u32;
      /// Describes the encoding of the graph. This allows the API to be implemented by various
      /// backends that encode (i.e., serialize) their graph IR with different formats.
      #[repr(u8)]
      #[derive(Clone, Copy, Eq, PartialEq)]
      pub enum GraphEncoding {
        Openvino,
        Onnx,
        Tensorflow,
        Pytorch,
        Tensorflowlite,
        Autodetect,
      }
      impl ::core::fmt::Debug for GraphEncoding {
        fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
          match self {
            GraphEncoding::Openvino => {
              f.debug_tuple("GraphEncoding::Openvino").finish()
            }
            GraphEncoding::Onnx => {
              f.debug_tuple("GraphEncoding::Onnx").finish()
            }
            GraphEncoding::Tensorflow => {
              f.debug_tuple("GraphEncoding::Tensorflow").finish()
            }
            GraphEncoding::Pytorch => {
              f.debug_tuple("GraphEncoding::Pytorch").finish()
            }
            GraphEncoding::Tensorflowlite => {
              f.debug_tuple("GraphEncoding::Tensorflowlite").finish()
            }
            GraphEncoding::Autodetect => {
              f.debug_tuple("GraphEncoding::Autodetect").finish()
            }
          }
        }
      }

      impl GraphEncoding{
        pub(crate) unsafe fn _lift(val: u8) -> GraphEncoding{
          if !cfg!(debug_assertions) {
            return ::core::mem::transmute(val);
          }

          match val {
            0 => GraphEncoding::Openvino,
            1 => GraphEncoding::Onnx,
            2 => GraphEncoding::Tensorflow,
            3 => GraphEncoding::Pytorch,
            4 => GraphEncoding::Tensorflowlite,
            5 => GraphEncoding::Autodetect,

            _ => panic!("invalid enum discriminant"),
          }
        }
      }

      /// Define where the graph should be executed.
      #[repr(u8)]
      #[derive(Clone, Copy, Eq, PartialEq)]
      pub enum ExecutionTarget {
        Cpu,
        Gpu,
        Tpu,
      }
      impl ::core::fmt::Debug for ExecutionTarget {
        fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
          match self {
            ExecutionTarget::Cpu => {
              f.debug_tuple("ExecutionTarget::Cpu").finish()
            }
            ExecutionTarget::Gpu => {
              f.debug_tuple("ExecutionTarget::Gpu").finish()
            }
            ExecutionTarget::Tpu => {
              f.debug_tuple("ExecutionTarget::Tpu").finish()
            }
          }
        }
      }

      impl ExecutionTarget{
        pub(crate) unsafe fn _lift(val: u8) -> ExecutionTarget{
          if !cfg!(debug_assertions) {
            return ::core::mem::transmute(val);
          }

          match val {
            0 => ExecutionTarget::Cpu,
            1 => ExecutionTarget::Gpu,
            2 => ExecutionTarget::Tpu,

            _ => panic!("invalid enum discriminant"),
          }
        }
      }

      /// The graph initialization data.
      ///
      /// This gets bundled up into an array of buffers because implementing backends may encode their
      /// graph IR in parts (e.g., OpenVINO stores its IR and weights separately).
      pub type GraphBuilder = _rt::Vec::<u8>;
      #[allow(unused_unsafe, clippy::all)]
      /// Load a `graph` from an opaque sequence of bytes to use for inference.
      pub fn load(builder: &[GraphBuilder],encoding: GraphEncoding,target: ExecutionTarget,) -> Result<Graph,Error>{
        unsafe {
          #[repr(align(4))]
          struct RetArea([::core::mem::MaybeUninit::<u8>; 8]);
          let mut ret_area = RetArea([::core::mem::MaybeUninit::uninit(); 8]);
          let vec1 = builder;
          let len1 = vec1.len();
          let layout1 = _rt::alloc::Layout::from_size_align_unchecked(vec1.len() * 8, 4);
          let result1 = if layout1.size() != 0 {
            let ptr = _rt::alloc::alloc(layout1).cast::<u8>();
            if ptr.is_null()
            {
              _rt::alloc::handle_alloc_error(layout1);
            }
            ptr
          }else {{
            ::core::ptr::null_mut()
          }};
          for (i, e) in vec1.into_iter().enumerate() {
            let base = result1.add(i * 8);
            {
              let vec0 = e;
              let ptr0 = vec0.as_ptr().cast::<u8>();
              let len0 = vec0.len();
              *base.add(4).cast::<usize>() = len0;
              *base.add(0).cast::<*mut u8>() = ptr0.cast_mut();
            }
          }
          let ptr2 = ret_area.0.as_mut_ptr().cast::<u8>();
          #[cfg(target_arch = "wasm32")]
          #[link(wasm_import_module = "wasi:nn/graph")]
          extern "C" {
            #[link_name = "load"]
            fn wit_import(_: *mut u8, _: usize, _: i32, _: i32, _: *mut u8, );
          }

          #[cfg(not(target_arch = "wasm32"))]
          fn wit_import(_: *mut u8, _: usize, _: i32, _: i32, _: *mut u8, ){ unreachable!() }
          wit_import(result1, len1, encoding.clone() as i32, target.clone() as i32, ptr2);
          let l3 = i32::from(*ptr2.add(0).cast::<u8>());
          if layout1.size() != 0 {
            _rt::alloc::dealloc(result1.cast(), layout1);
          }
          match l3 {
            0 => {
              let e = {
                let l4 = *ptr2.add(4).cast::<i32>();

                l4 as u32
              };
              Ok(e)
            }
            1 => {
              let e = {
                let l5 = i32::from(*ptr2.add(4).cast::<u8>());

                super::super::super::wasi::nn::errors::Error::_lift(l5 as u8)
              };
              Err(e)
            }
            _ => _rt::invalid_enum_discriminant(),
          }
        }
      }
      #[allow(unused_unsafe, clippy::all)]
      /// Load a `graph` by name.
      ///
      /// How the host expects the names to be passed and how it stores the graphs for retrieval via
      /// this function is **implementation-specific**. This allows hosts to choose name schemes that
      /// range from simple to complex (e.g., URLs?) and caching mechanisms of various kinds.
      pub fn load_by_name(name: &str,) -> Result<Graph,Error>{
        unsafe {
          #[repr(align(4))]
          struct RetArea([::core::mem::MaybeUninit::<u8>; 8]);
          let mut ret_area = RetArea([::core::mem::MaybeUninit::uninit(); 8]);
          let vec0 = name;
          let ptr0 = vec0.as_ptr().cast::<u8>();
          let len0 = vec0.len();
          let ptr1 = ret_area.0.as_mut_ptr().cast::<u8>();
          #[cfg(target_arch = "wasm32")]
          #[link(wasm_import_module = "wasi:nn/graph")]
          extern "C" {
            #[link_name = "load-by-name"]
            fn wit_import(_: *mut u8, _: usize, _: *mut u8, );
          }

          #[cfg(not(target_arch = "wasm32"))]
          fn wit_import(_: *mut u8, _: usize, _: *mut u8, ){ unreachable!() }
          wit_import(ptr0.cast_mut(), len0, ptr1);
          let l2 = i32::from(*ptr1.add(0).cast::<u8>());
          match l2 {
            0 => {
              let e = {
                let l3 = *ptr1.add(4).cast::<i32>();

                l3 as u32
              };
              Ok(e)
            }
            1 => {
              let e = {
                let l4 = i32::from(*ptr1.add(4).cast::<u8>());

                super::super::super::wasi::nn::errors::Error::_lift(l4 as u8)
              };
              Err(e)
            }
            _ => _rt::invalid_enum_discriminant(),
          }
        }
      }

    }

    #[allow(clippy::all)]
    pub mod inference {
      #[used]
      #[doc(hidden)]
      #[cfg(target_arch = "wasm32")]
      static __FORCE_SECTION_REF: fn() = super::super::super::__link_custom_section_describing_imports;
      use super::super::super::_rt;
      pub type Error = super::super::super::wasi::nn::errors::Error;
      pub type Tensor = super::super::super::wasi::nn::tensor::Tensor;
      pub type TensorData = super::super::super::wasi::nn::tensor::TensorData;
      pub type Graph = super::super::super::wasi::nn::graph::Graph;
      /// Bind a `graph` to the input and output tensors for an inference.
      ///
      /// TODO: this is no longer necessary in WIT (https://github.com/WebAssembly/wasi-nn/issues/43)
      pub type GraphExecutionContext = u32;
      #[allow(unused_unsafe, clippy::all)]
      /// Create an execution instance of a loaded graph.
      pub fn init_execution_context(graph: Graph,) -> Result<GraphExecutionContext,Error>{
        unsafe {
          #[repr(align(4))]
          struct RetArea([::core::mem::MaybeUninit::<u8>; 8]);
          let mut ret_area = RetArea([::core::mem::MaybeUninit::uninit(); 8]);
          let ptr0 = ret_area.0.as_mut_ptr().cast::<u8>();
          #[cfg(target_arch = "wasm32")]
          #[link(wasm_import_module = "wasi:nn/inference")]
          extern "C" {
            #[link_name = "init-execution-context"]
            fn wit_import(_: i32, _: *mut u8, );
          }

          #[cfg(not(target_arch = "wasm32"))]
          fn wit_import(_: i32, _: *mut u8, ){ unreachable!() }
          wit_import(_rt::as_i32(graph), ptr0);
          let l1 = i32::from(*ptr0.add(0).cast::<u8>());
          match l1 {
            0 => {
              let e = {
                let l2 = *ptr0.add(4).cast::<i32>();

                l2 as u32
              };
              Ok(e)
            }
            1 => {
              let e = {
                let l3 = i32::from(*ptr0.add(4).cast::<u8>());

                super::super::super::wasi::nn::errors::Error::_lift(l3 as u8)
              };
              Err(e)
            }
            _ => _rt::invalid_enum_discriminant(),
          }
        }
      }
      #[allow(unused_unsafe, clippy::all)]
      /// Define the inputs to use for inference.
      pub fn set_input(ctx: GraphExecutionContext,index: u32,tensor: &Tensor,) -> Result<(),Error>{
        unsafe {
          #[repr(align(1))]
          struct RetArea([::core::mem::MaybeUninit::<u8>; 2]);
          let mut ret_area = RetArea([::core::mem::MaybeUninit::uninit(); 2]);
          let super::super::super::wasi::nn::tensor::Tensor{ dimensions:dimensions0, tensor_type:tensor_type0, data:data0, } = tensor;
          let vec1 = dimensions0;
          let ptr1 = vec1.as_ptr().cast::<u8>();
          let len1 = vec1.len();
          let vec2 = data0;
          let ptr2 = vec2.as_ptr().cast::<u8>();
          let len2 = vec2.len();
          let ptr3 = ret_area.0.as_mut_ptr().cast::<u8>();
          #[cfg(target_arch = "wasm32")]
          #[link(wasm_import_module = "wasi:nn/inference")]
          extern "C" {
            #[link_name = "set-input"]
            fn wit_import(_: i32, _: i32, _: *mut u8, _: usize, _: i32, _: *mut u8, _: usize, _: *mut u8, );
          }

          #[cfg(not(target_arch = "wasm32"))]
          fn wit_import(_: i32, _: i32, _: *mut u8, _: usize, _: i32, _: *mut u8, _: usize, _: *mut u8, ){ unreachable!() }
          wit_import(_rt::as_i32(ctx), _rt::as_i32(&index), ptr1.cast_mut(), len1, tensor_type0.clone() as i32, ptr2.cast_mut(), len2, ptr3);
          let l4 = i32::from(*ptr3.add(0).cast::<u8>());
          match l4 {
            0 => {
              let e = ();
              Ok(e)
            }
            1 => {
              let e = {
                let l5 = i32::from(*ptr3.add(1).cast::<u8>());

                super::super::super::wasi::nn::errors::Error::_lift(l5 as u8)
              };
              Err(e)
            }
            _ => _rt::invalid_enum_discriminant(),
          }
        }
      }
      #[allow(unused_unsafe, clippy::all)]
      /// Compute the inference on the given inputs.
      ///
      /// Note the expected sequence of calls: `set-input`, `compute`, `get-output`. TODO: this
      /// expectation could be removed as a part of https://github.com/WebAssembly/wasi-nn/issues/43.
      pub fn compute(ctx: GraphExecutionContext,) -> Result<(),Error>{
        unsafe {
          #[repr(align(1))]
          struct RetArea([::core::mem::MaybeUninit::<u8>; 2]);
          let mut ret_area = RetArea([::core::mem::MaybeUninit::uninit(); 2]);
          let ptr0 = ret_area.0.as_mut_ptr().cast::<u8>();
          #[cfg(target_arch = "wasm32")]
          #[link(wasm_import_module = "wasi:nn/inference")]
          extern "C" {
            #[link_name = "compute"]
            fn wit_import(_: i32, _: *mut u8, );
          }

          #[cfg(not(target_arch = "wasm32"))]
          fn wit_import(_: i32, _: *mut u8, ){ unreachable!() }
          wit_import(_rt::as_i32(ctx), ptr0);
          let l1 = i32::from(*ptr0.add(0).cast::<u8>());
          match l1 {
            0 => {
              let e = ();
              Ok(e)
            }
            1 => {
              let e = {
                let l2 = i32::from(*ptr0.add(1).cast::<u8>());

                super::super::super::wasi::nn::errors::Error::_lift(l2 as u8)
              };
              Err(e)
            }
            _ => _rt::invalid_enum_discriminant(),
          }
        }
      }
      #[allow(unused_unsafe, clippy::all)]
      /// Extract the outputs after inference.
      pub fn get_output(ctx: GraphExecutionContext,index: u32,) -> Result<TensorData,Error>{
        unsafe {
          #[repr(align(4))]
          struct RetArea([::core::mem::MaybeUninit::<u8>; 12]);
          let mut ret_area = RetArea([::core::mem::MaybeUninit::uninit(); 12]);
          let ptr0 = ret_area.0.as_mut_ptr().cast::<u8>();
          #[cfg(target_arch = "wasm32")]
          #[link(wasm_import_module = "wasi:nn/inference")]
          extern "C" {
            #[link_name = "get-output"]
            fn wit_import(_: i32, _: i32, _: *mut u8, );
          }

          #[cfg(not(target_arch = "wasm32"))]
          fn wit_import(_: i32, _: i32, _: *mut u8, ){ unreachable!() }
          wit_import(_rt::as_i32(ctx), _rt::as_i32(&index), ptr0);
          let l1 = i32::from(*ptr0.add(0).cast::<u8>());
          match l1 {
            0 => {
              let e = {
                let l2 = *ptr0.add(4).cast::<*mut u8>();
                let l3 = *ptr0.add(8).cast::<usize>();
                let len4 = l3;

                _rt::Vec::from_raw_parts(l2.cast(), len4, len4)
              };
              Ok(e)
            }
            1 => {
              let e = {
                let l5 = i32::from(*ptr0.add(4).cast::<u8>());

                super::super::super::wasi::nn::errors::Error::_lift(l5 as u8)
              };
              Err(e)
            }
            _ => _rt::invalid_enum_discriminant(),
          }
        }
      }

    }

  }
}
mod _rt {
  pub use alloc_crate::vec::Vec;
  pub use alloc_crate::alloc;
  pub unsafe fn invalid_enum_discriminant<T>() -> T {
    if cfg!(debug_assertions) {
      panic!("invalid enum discriminant")
    } else {
      core::hint::unreachable_unchecked()
    }
  }
  
  pub fn as_i32<T: AsI32>(t: T) -> i32 {
    t.as_i32()
  }

  pub trait AsI32 {
    fn as_i32(self) -> i32;
  }

  impl<'a, T: Copy + AsI32> AsI32 for &'a T {
    fn as_i32(self) -> i32 {
      (*self).as_i32()
    }
  }
  
  impl AsI32 for i32 {
    #[inline]
    fn as_i32(self) -> i32 {
      self as i32
    }
  }
  
  impl AsI32 for u32 {
    #[inline]
    fn as_i32(self) -> i32 {
      self as i32
    }
  }
  
  impl AsI32 for i16 {
    #[inline]
    fn as_i32(self) -> i32 {
      self as i32
    }
  }
  
  impl AsI32 for u16 {
    #[inline]
    fn as_i32(self) -> i32 {
      self as i32
    }
  }
  
  impl AsI32 for i8 {
    #[inline]
    fn as_i32(self) -> i32 {
      self as i32
    }
  }
  
  impl AsI32 for u8 {
    #[inline]
    fn as_i32(self) -> i32 {
      self as i32
    }
  }
  
  impl AsI32 for char {
    #[inline]
    fn as_i32(self) -> i32 {
      self as i32
    }
  }
  
  impl AsI32 for usize {
    #[inline]
    fn as_i32(self) -> i32 {
      self as i32
    }
  }
  extern crate alloc as alloc_crate;
}

#[cfg(target_arch = "wasm32")]
#[link_section = "component-type:wit-bindgen:0.22.0:ml:encoded world"]
#[doc(hidden)]
pub static __WIT_BINDGEN_COMPONENT_TYPE: [u8; 1047] = *b"\
\0asm\x0d\0\x01\0\0\x19\x16wit-component-encoding\x04\0\x07\x9e\x07\x01A\x02\x01\
A\x0c\x01B\x08\x01py\x04\0\x11tensor-dimensions\x03\0\0\x01m\x07\x04FP16\x04FP32\
\x04FP64\x04BF16\x02U8\x03I32\x03I64\x04\0\x0btensor-type\x03\0\x02\x01p}\x04\0\x0b\
tensor-data\x03\0\x04\x01r\x03\x0adimensions\x01\x0btensor-type\x03\x04data\x05\x04\
\0\x06tensor\x03\0\x06\x03\x01\x0ewasi:nn/tensor\x05\0\x01B\x02\x01m\x07\x10inva\
lid-argument\x10invalid-encoding\x04busy\x0druntime-error\x15unsupported-operati\
on\x09too-large\x09not-found\x04\0\x05error\x03\0\0\x03\x01\x0ewasi:nn/errors\x05\
\x01\x02\x03\0\x01\x05error\x02\x03\0\0\x06tensor\x01B\x12\x02\x03\x02\x01\x02\x04\
\0\x05error\x03\0\0\x02\x03\x02\x01\x03\x04\0\x06tensor\x03\0\x02\x01y\x04\0\x05\
graph\x03\0\x04\x01m\x06\x08openvino\x04onnx\x0atensorflow\x07pytorch\x0etensorf\
lowlite\x0aautodetect\x04\0\x0egraph-encoding\x03\0\x06\x01m\x03\x03cpu\x03gpu\x03\
tpu\x04\0\x10execution-target\x03\0\x08\x01p}\x04\0\x0dgraph-builder\x03\0\x0a\x01\
p\x0b\x01j\x01\x05\x01\x01\x01@\x03\x07builder\x0c\x08encoding\x07\x06target\x09\
\0\x0d\x04\0\x04load\x01\x0e\x01@\x01\x04names\0\x0d\x04\0\x0cload-by-name\x01\x0f\
\x03\x01\x0dwasi:nn/graph\x05\x04\x02\x03\0\0\x0btensor-data\x02\x03\0\x02\x05gr\
aph\x01B\x15\x02\x03\x02\x01\x02\x04\0\x05error\x03\0\0\x02\x03\x02\x01\x03\x04\0\
\x06tensor\x03\0\x02\x02\x03\x02\x01\x05\x04\0\x0btensor-data\x03\0\x04\x02\x03\x02\
\x01\x06\x04\0\x05graph\x03\0\x06\x01y\x04\0\x17graph-execution-context\x03\0\x08\
\x01j\x01\x09\x01\x01\x01@\x01\x05graph\x07\0\x0a\x04\0\x16init-execution-contex\
t\x01\x0b\x01j\0\x01\x01\x01@\x03\x03ctx\x09\x05indexy\x06tensor\x03\0\x0c\x04\0\
\x09set-input\x01\x0d\x01@\x01\x03ctx\x09\0\x0c\x04\0\x07compute\x01\x0e\x01j\x01\
\x05\x01\x01\x01@\x02\x03ctx\x09\x05indexy\0\x0f\x04\0\x0aget-output\x01\x10\x03\
\x01\x11wasi:nn/inference\x05\x07\x04\x01\x0awasi:nn/ml\x04\0\x0b\x08\x01\0\x02m\
l\x03\0\0\0G\x09producers\x01\x0cprocessed-by\x02\x0dwit-component\x070.201.0\x10\
wit-bindgen-rust\x060.22.0";

#[inline(never)]
#[doc(hidden)]
#[cfg(target_arch = "wasm32")]
pub fn __link_custom_section_describing_imports() {
  wit_bindgen::rt::maybe_link_cabi_realloc();
}

